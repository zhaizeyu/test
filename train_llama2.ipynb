{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONuakSxk0UmzdZdlVQ11PO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhaizeyu/test/blob/master/train_llama2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MboNQ6xgJYZa"
      },
      "outputs": [],
      "source": [
        "!mkdir -p '/root/.kaggle'\n",
        "from google.colab import userdata\n",
        "\n",
        "# 打开文件以写入，如果文件不存在则创建\n",
        "with open('/root/.kaggle/kaggle.json','w') as file:\n",
        "    f = {\"username\":userdata.get('KAGGLE_USERNAME'),\"key\":userdata.get('KAGGLE_KEY')}\n",
        "    # 写入内容 'ccc' 到文件\n",
        "    file.write('{\"username\":\"zeyuzhai\",\"key\":\"42dc12af61163cb7e7c0a849dd7e8f49\"}')\n",
        "!chmod 600 '/root/.kaggle/kaggle.json'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d nbroad/gemma-rewrite-nbroad"
      ],
      "metadata": {
        "id": "zHLmeO7kMbW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gemma-rewrite-nbroad.zip"
      ],
      "metadata": {
        "id": "AVVbDKDGMd4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ],
      "metadata": {
        "id": "CFn9CaxNMggt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "tKXXgPU8MiFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"llama-2-7b-recover\"\n",
        "\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 25\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "\n",
        "device_map = 'auto'"
      ],
      "metadata": {
        "id": "S-B2_e5QMkBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_data_path='nbroad-v1.csv'\n",
        "ori_data = pd.read_csv(train_data_path, usecols=['id', 'original_text', 'rewrite_prompt', 'rewritten_text'])\n"
      ],
      "metadata": {
        "id": "evsPidZbMnS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "    Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\n",
        "    You are trying to understand how the original essay was transformed into a new version.\n",
        "    Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\n",
        "    Start directly with the prompt, that's all I need. Output should be only line ONLY.\n",
        "<</SYS>>\n",
        "    Original Essay:\n",
        "    {original_text}\n",
        "\n",
        "    Rewritten Essay:\n",
        "    {rewritten_text}\n",
        " [/INST]\n",
        "    {rewrite_prompt}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "j-eA8LiLMou3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ori_data[\"promot\"] = ori_data.apply(lambda row: template.format(original_text=row.original_text,\n",
        "                                                             rewritten_text=row.rewritten_text,\n",
        "                                                             rewrite_prompt=row.rewrite_prompt), axis=1)\n",
        "\n",
        "ori_data.drop(['id', 'original_text', 'rewritten_text','rewrite_prompt'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "y6yn3c8DMqi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ori_data"
      ],
      "metadata": {
        "id": "pm14qrvuMsn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E3RH-cs-M06l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "# Load dataset (you can process it here)\n",
        "dataset = Dataset.from_pandas(ori_data)"
      ],
      "metadata": {
        "id": "c2xluu20MMwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "t0UYVrgDOL4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urldata = load_dataset(dataset_name, split=\"train\")\n",
        "urldata['text'][0]"
      ],
      "metadata": {
        "id": "xlVJ01E6R8TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n"
      ],
      "metadata": {
        "id": "OJXpOgBFuSrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"promot\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n"
      ],
      "metadata": {
        "id": "hkisRkVEiwhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "id": "WnFIjH-dXzuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir results/runs"
      ],
      "metadata": {
        "id": "crj9svNe4hU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"What is a large language model?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "frlSLPin4IJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty VRAM\n",
        "del model\n",
        "del pipe\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "mkQCviG0Zta-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "QQn30cRtAZ-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !huggingface-cli login"
      ],
      "metadata": {
        "id": "dk8KVLszciWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(new_model, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
      ],
      "metadata": {
        "id": "x-xPb-_qB0dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "模型验证"
      ],
      "metadata": {
        "id": "YGmZ1dlhlvPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_data_path='nbroad-v2.csv'\n",
        "test_data = pd.read_csv(train_data_path, usecols=['original_text', 'rewrite_prompt', 'rewritten_text'])"
      ],
      "metadata": {
        "id": "nwc5-SR9niCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_txt(text, length):\n",
        "    text_list = text.split()\n",
        "\n",
        "    if len(text_list) <= length:\n",
        "        return text\n",
        "\n",
        "    return \" \".join(text_list[:length])\n",
        "\n",
        "\n",
        "def gen_prompt(og_text, rewritten_text):\n",
        "    # Truncate the texts to first 200 words for now\n",
        "    # As we are having memory issues on Mixtral8x7b\n",
        "    og_text = truncate_txt(og_text,200)\n",
        "    rewritten_text = truncate_txt(rewritten_text,200)\n",
        "    promot_tamplate = f\"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "    Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\n",
        "    You are trying to understand how the original essay was transformed into a new version.\n",
        "    Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\n",
        "    Start directly with the prompt, that's all I need. Output should be only line ONLY.\n",
        "<</SYS>>\n",
        "    Original Essay:\n",
        "    \\\"\"\"{og_text}\\\"\"\"\n",
        "\n",
        "    Rewritten Essay:\n",
        "    \\\"\"\"{rewritten_text}\\\"\"\"\n",
        " [/INST]\n",
        "\"\"\"\n",
        "    return promot_tamplate\n",
        "    # return f\"\"\"\n",
        "    # Original Essay:\n",
        "    # \\\"\"\"{og_text}\\\"\"\"\n",
        "\n",
        "    # Rewritten Essay:\n",
        "    # \\\"\"\"{rewritten_text}\\\"\"\"\n",
        "\n",
        "    # Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\n",
        "    # You are trying to understand how the original essay was transformed into a new version.\n",
        "    # Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\n",
        "    # Start directly with the prompt, that's all I need. Output should be only line ONLY.\n",
        "    # \"\"\""
      ],
      "metadata": {
        "id": "H-c0h2Zmsrd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data=test_data[:5]\n",
        "test_data[\"promot\"] = test_data.apply(lambda row: gen_prompt(row.original_text,rewritten_text=row.rewritten_text), axis=1)\n"
      ],
      "metadata": {
        "id": "7GXfLYQAEKYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "5r1EdtJLLie-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "promot=test_data[\"promot\"][4]\n",
        "promot"
      ],
      "metadata": {
        "id": "hHdPXG-rwh6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_memory():\n",
        "  import gc\n",
        "  import ctypes\n",
        "  gc.collect()\n",
        "  ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
        "  torch.cuda.empty_cache()\n",
        "def model_do(promot):\n",
        "  clean_memory()\n",
        "  global tokenizer\n",
        "  global model\n",
        "  inputs = tokenizer(promot, return_tensors=\"pt\").to('cuda')\n",
        "# Generate\n",
        "  generate_ids = model.generate(inputs.input_ids, max_length=1000)\n",
        "  decoded_output=tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "  # if \"Prompt:\" in decoded_output:\n",
        "  #   result = decoded_output[last_prompt_index + len(\"Prompt:\"):].strip()\n",
        "  # else:\n",
        "  #   DEFAULT_TEXT = \"Please improve the following text using the writing style of, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.Enhance the clarity, elegance, and impact of the following text by adopting the writing style of , ensuring the core message remains intact while transforming the tone, word choice, and stylistic features to align with the specified style.\"\n",
        "  #   result = DEFAULT_TEXT\n",
        "  import re\n",
        "  decoded_output = re.sub(r\"[\\s\\S]*\\[\\/INST\\]\", '', decoded_output, 1)\n",
        "  return decoded_output\n"
      ],
      "metadata": {
        "id": "x1xaxOYm5Vm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = model_do(promot)\n",
        "res"
      ],
      "metadata": {
        "id": "xArUY5HY_Ox5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[\"pred\"] = test_data.apply(lambda row: model_do(row.promot), axis=1)"
      ],
      "metadata": {
        "id": "8FVep0ul0sjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.drop(['original_text','rewritten_text'], axis=1, inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "tgSYrT-mDcuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "O1SKNDhSl97n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.to_csv('output.csv',encoding='utf-8', index=False)\n"
      ],
      "metadata": {
        "id": "mIx3FygCQLM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "模型打分"
      ],
      "metadata": {
        "id": "3nZSXxEAUb92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d yeoyunsianggeremie/sentence-transformers-2-4-0"
      ],
      "metadata": {
        "id": "qxA3O9V9hhzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q sentence-transformers-2-4-0.zip"
      ],
      "metadata": {
        "id": "lDWRLO5S3DdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d yeoyunsianggeremie/sentence-t5-base-hf"
      ],
      "metadata": {
        "id": "ZiWizM894zm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip sentence-t5-base-hf.zip"
      ],
      "metadata": {
        "id": "gmSdgShg45In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq transformers==4.36.0"
      ],
      "metadata": {
        "id": "I8_83BOX4Fcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq sentence_transformers-2.4.0-py3-none-any.whl"
      ],
      "metadata": {
        "id": "XZGMeOcE22LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "AiFjw_UNhGi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(\"output.csv\")"
      ],
      "metadata": {
        "id": "i1JF_6bThzAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CVScore(test):\n",
        "\n",
        "    scs = lambda row: abs((cosine_similarity(row[\"actual_embeddings\"], row[\"pred_embeddings\"])) ** 3)\n",
        "\n",
        "    model = SentenceTransformer('sentence-t5-base')\n",
        "\n",
        "    test[\"actual_embeddings\"] = test[\"rewrite_prompt\"].progress_apply(lambda x: model.encode(x, normalize_embeddings=True, show_progress_bar=False).reshape(1, -1))\n",
        "    test[\"pred_embeddings\"] = test[\"pred\"].progress_apply(lambda x: model.encode(x, normalize_embeddings=True, show_progress_bar=False).reshape(1, -1))\n",
        "\n",
        "    test[\"score\"] = test.apply(scs, axis=1)\n",
        "    for i in test[\"score\"]:\n",
        "      print(i)\n",
        "    return np.mean(test['score'])[0][0]\n",
        "\n",
        "print(f\"CV Score: {CVScore(test)}\")"
      ],
      "metadata": {
        "id": "hMqh-Bw_h1SB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}